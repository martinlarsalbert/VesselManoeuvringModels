{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b04afd31-695a-4acf-bdf4-992e882c8477",
   "metadata": {},
   "source": [
    "# Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38130ef-9858-44a4-a057-f6e6e3e51a54",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.optimize import minimize\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "from myst_nb import glue\n",
    "\n",
    "import os\n",
    "if os.name == 'nt':\n",
    "    plt.style.use('book.mplstyle')  # Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba169149-d401-40d4-be9d-c36a6434ab9b",
   "metadata": {},
   "source": [
    "Let's consider the simplest possible scenario, where some force $f_y$ is modelled as function of velocity $v$ and some hydrodynamic coefficient $\\beta$:\n",
    "\n",
    "$$ f_y = \\beta \\cdot v  $$ (eq_model)\n",
    "\n",
    "One physical experiment is carried out where the force $f_y$ is measured at a certain speed $v$. (We also measure that there is no force at rest ($v=0$) to confirm the lack of interception term in the model {eq}`eq_model`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c239dbba-30f5-459f-9472-92576f0a3c70",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# generate data\n",
    "np.random.seed(42)\n",
    "N = 10\n",
    "\n",
    "beta = 3\n",
    "scale = 0.5\n",
    "\n",
    "data = pd.DataFrame()\n",
    "v = data['v'] = np.linspace(0,5,N)\n",
    "ϵ = data['ϵ'] = np.random.normal(loc = 0.0, scale = scale, size = N)\n",
    "\n",
    "f_y = data['f_y'] = beta*v\n",
    "f_y_measure = data['f_y_measure'] = f_y + ϵ\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d22eab8-5cef-4417-b8d5-2975e8d6116d",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "n = 3\n",
    "f_y_sample = f_y_measure[n]\n",
    "v_sample = v[n]\n",
    "beta_hat = f_y_sample/v_sample\n",
    "glue(\"f_y_sample\", np.round(f_y_sample, 2), display=False)\n",
    "glue(\"v_sample\", np.round(v_sample, 2) , display=False)\n",
    "glue(\"beta_hat\", np.round(beta_hat, 2), display=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded2b832-2322-48e6-b5a2-0624aa2ca310",
   "metadata": {
    "tags": [
     "remove_input",
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "data['beta'] = data['f_y_measure'] / data['v']\n",
    "glue(\"tab_experiments\", data[['v','f_y_measure','beta']].round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e090344b-6da5-42c8-aae1-7a928822ca12",
   "metadata": {},
   "source": [
    "{glue:}`f_y_sample` [N] force ($f_y$) was measured during the conducted experiment at a speed ($v$) of {glue:}`v_sample` [m/s].  \n",
    "As the model {eq}`eq_model` contains only one unknown parameter: $\\beta$ this one only experiment is enought to determine $\\beta$:\n",
    "\n",
    "$$\\beta = \\frac{f_y}{v} $$ (eq_beta_deterministic)\n",
    "\n",
    "So that beta can be estimated as {glue:}`beta_hat`.\n",
    "\n",
    "If the measurement was perfect and the used model describes the physics perfectly this estimation of $\\beta$ is the correct one. In order to double check this several experiment was conducted, as seen in the table below:\n",
    "\n",
    "\n",
    "```{glue:figure} tab_experiments\n",
    ":figwidth: 300px\n",
    ":name: \"tab_experiments\"\n",
    "Result from experiments\n",
    "```\n",
    "\n",
    "It can be seen that {eq}`eq_beta_deterministic` gives different estimates of $\\beta$ from the different experiments. So there must be some measurement errors or model errors (or booth) in the data from these experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bf2439-1064-4a6c-9ea7-944bbfa23d0e",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "data.plot(x='v', y='f_y_measure', style='bo', ax=ax);\n",
    "ax.set_ylabel(r'$f_y$')\n",
    "ax.set_xlabel(r'$v$');\n",
    "ax.get_legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6dc733-0e8e-4838-bfce-7b00519a96f1",
   "metadata": {},
   "source": [
    "If it is asumed that the deviations comes from measurement errors, it is common to assume that this error $\\epsilon$ follows the normal distribution with zero mean and some standard deviation $\\sigma$:\n",
    "\n",
    "$$ \\epsilon \\sim \\mathcal{N}(0,\\,\\sigma^{2}) $$\n",
    "\n",
    "The figure below shows a probability density plot (PDF) of a normal distribution for measurement error. It can be seen that having no error $\\epsilon=0$ has the highest probability, and values very far away from zero, like +/- 2 N have very low probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072fcd58-febf-41f3-9ff3-1945ba3826fa",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "ϵ = stats.norm(loc=0, scale=scale)\n",
    "\n",
    "epsilons = np.linspace(-2.5*scale,2.5*scale,100)\n",
    "p = ϵ.pdf(epsilons)\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(epsilons,p);\n",
    "ax.set_xlabel(r'$\\epsilon$ (measurement error)');\n",
    "ax.set_ylabel(r'$P$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fb5303-46c8-4acf-bbae-425892293a44",
   "metadata": {},
   "source": [
    "The measurement error can now be added to the estimation of $f_y$ by modifying the model equation ({eq}`eq_model`):\n",
    "\n",
    "$$ f_y = \\beta \\cdot v + \\epsilon $$ (eq_model_probabalisic)\n",
    "\n",
    "The regression problem can now be solved if we can find the normal distribution that created the measurement noise as seen in the figure below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fe4b6d-3e4d-4a5e-a5d7-a41305bda19d",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "data.plot(x='v', y='f_y_measure', style='bo', label='measurements', ax=ax)\n",
    "data.plot(x='v', y='f_y', style='g--o', label='estimate', ax=ax)\n",
    "\n",
    "p_ = p/3  # Scaling for figure\n",
    "p_max = np.max(p_)\n",
    "\n",
    "\n",
    "for index,row in data.iterrows():\n",
    "    ax.plot(row['v'] + p_ - p_max, row['f_y'] + epsilons, 'k-', zorder=-10)\n",
    "    ax.plot([row['v'],row['v']], [row['f_y'],row['f_y_measure']], 'r-') \n",
    "\n",
    "ax.set_xlabel(r'$v$');\n",
    "ax.set_ylabel(r'$f_y$');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f7c61-fe00-4329-9172-1813e0de786e",
   "metadata": {},
   "source": [
    "We assume that the measurement error distribution is the same for all measurements as seen in the figure above. It does not matter if the speed $v$ is 2 or 5 m/s, we still expect the error to come from the same distibution. And this is where the Maximum Likelihood Estimation comes into play. We want to find the distribution that it is most likely that the measurement error in our data originate from.\n",
    "\n",
    "The likelihood for one arbitrary distribution is calculated as a product of the likelihoods (y-axis of the PDF) for all data points as seen in the below figure: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e59cb52-1068-4c6a-b1d5-c06c40e2d285",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(-2.5*scale,2.5*scale,100)\n",
    "p = stats.norm.pdf(epsilons, loc=0, scale=scale)\n",
    "\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(epsilons,p);\n",
    "\n",
    "data['P'] = ϵ.pdf(data['ϵ'])\n",
    "\n",
    "data.plot(x='ϵ', y='P', ax=ax, style='bo')\n",
    "for index,row in data.iterrows():\n",
    "    ax.text(x=row['ϵ']+0.05, y=row['P']-0.01, s=f'P{index}')\n",
    "\n",
    "\n",
    "ax.set_xlabel(r'$\\epsilon$ (measurement error)');\n",
    "ax.set_ylabel(r'$P$');\n",
    "ax.get_legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c82d12-d72c-45f6-b3d0-7148d4d6d137",
   "metadata": {},
   "source": [
    "This can be implemented as a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8e9cf4-d3ca-46b1-8668-0bf47b840b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood = 1\n",
    "for P in data['P']:\n",
    "    likelihood*=P\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e126c4-a2db-4ae8-80b5-57bde4e85a0f",
   "metadata": {},
   "source": [
    "This kind of itterative product very quickly gets out of hand, if there is thousands or millions of data points the calculation gets very complicated. So there is a mathematical \"trick\" to instead used the log(PDF) where the products instead becomes a summation, which is much easier to work with.\n",
    "\n",
    "The likelihood (or rather log-likelihood) that the data origins from a certain distribution can be calculated in tis way. But how do we know that this is the most likely? Perhas there is another distribution that is more likely than the one we tested above? This turns into an optimization problem, maximizing the likelihood which in the case of a normal distribution with zero mean boils down to determine the standard deviation $\\sigma$ of the distribution with highest likelihood. An analytical solution to the optimization problem can be found by finding the point where all the partial derivatives are zero. But instead we will use *scipy.optimize.minimize* to find a numerical solution instead. (The maximization problem needs to be reformulated into a minimization problem of the negative log-likelihood)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ced2413-0e8d-4518-bf9c-13fd5f1b441b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define likelihood function\n",
    "def MLERegression(params, v, f_y):\n",
    "    \n",
    "    # Parameter to be optimized:\n",
    "    # beta (slope of curve) and \n",
    "    # std (standard deviation of the error distribution)\n",
    "    beta, std = params[0], params[1]\n",
    "    \n",
    "    # Random variable with \"guessed\" standard deviation:\n",
    "    ϵ = stats.norm(loc=0, scale=std)\n",
    "    \n",
    "    # The estimated forces with \"guessed\" value of beta:\n",
    "    f_y_hat = beta*v\n",
    "    \n",
    "    # This estimation for the forces would give the following error:\n",
    "    errors = f_y - f_y_hat\n",
    "    \n",
    "    # The log-likelihoods of these errors can be calculated with the \n",
    "    # log of the ϵ PDF: \n",
    "    loglikelihoods = ϵ.logpdf(errors)\n",
    "    \n",
    "    # The total likelihood of observing the measured forces f_y can be calculated\n",
    "    # as the sum of all loglikelihoods:\n",
    "    likelihood = np.sum( loglikelihoods )\n",
    "    \n",
    "    # return negative LL\n",
    "    return(-likelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3334a353-48ae-4176-8e0b-a8eddb7b6d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let’s start with some random coefficient guesses and optimize\n",
    "guess = np.array([5,2])\n",
    "results = minimize(MLERegression, \n",
    "                   guess, \n",
    "                   args=(data['v'].values,data['f_y_measure'].values,), \n",
    "                   method = \"Nelder-Mead\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe7b18c-b0f8-444d-b865-263b3108ca8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9a8cfa-f538-4337-8b8d-af0493876fea",
   "metadata": {
    "tags": [
     "remove_cell"
    ]
   },
   "outputs": [],
   "source": [
    "beta_hat = results.x[0]\n",
    "scale_hat = results.x[1]\n",
    "data['f_y_hat'] = beta_hat*data['v']\n",
    "\n",
    "glue(\"beta_hat\", np.round(beta_hat, 2))\n",
    "glue(\"scale_hat\", np.round(scale_hat, 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42457edc-bdb8-4b0f-a0b6-cea0a98f3bf4",
   "metadata": {},
   "source": [
    "So the optimizer has found $\\hat{\\beta}$ = {glue:}`beta_hat` to be the most likely esimate for $\\beta$ and $\\hat{\\sigma}$ = {glue:}`scale_hat` to be the most likely standard deviation of the measurement error. \n",
    "In this toy example the experimental data was in fact not taken from a real physical experiment but was randomly generated using the assumed model {eq}`eq_model_probabalisic`. So we can have a look at how well the esimated slope and error distribution correspond to the real model that generated the data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e75e16b-c89b-47c1-b2ba-60e0b08975b8",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "data.plot(x='v', y='f_y_measure', style='bo', label='measurements', ax=ax);\n",
    "data.plot(x='v', y='f_y', style='k-', label='real $f_y$', lw=2, ax=ax);\n",
    "data.plot(x='v', y='f_y_hat', style='g-', label='estimated $f_y$', lw=2, ax=ax);\n",
    "\n",
    "ax.set_ylabel(r'$f_y$')\n",
    "ax.set_xlabel(r'$v$');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61097853-69ac-45f4-b821-812bae039452",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "epsilons = np.linspace(-2.5*scale,2.5*scale,100)\n",
    "p = stats.norm.pdf(epsilons, loc=0, scale=scale)\n",
    "fig,ax=plt.subplots()\n",
    "ax.plot(epsilons,p, label='real error distribution');\n",
    "\n",
    "p_estimate = stats.norm.pdf(epsilons, loc=0, scale=scale_hat)\n",
    "ax.plot(epsilons,p_estimate, label='estimated error distribution');\n",
    "\n",
    "\n",
    "\n",
    "data['P'] = ϵ.pdf(data['ϵ'])\n",
    "\n",
    "data.plot(x='ϵ', y='P', ax=ax, style='bo')\n",
    "for index,row in data.iterrows():\n",
    "    ax.text(x=row['ϵ']+0.05, y=row['P']-0.01, s=f'P{index}')\n",
    "\n",
    "\n",
    "ax.set_xlabel(r'$\\epsilon$ (measurement error)');\n",
    "ax.set_ylabel(r'$P$');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18fc01a-1e7d-4632-873e-95164393935d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Discussion\n",
    "The Maximum Likelihood Estimation seems to work well to regress the unknown parameter $\\beta$ on the measurement data. But this was a situation where the model {eq}`eq_model` choosen for the regression described the underlying physics really well (in fact it was the exact correct model). This is of course never the case when real data from real physical experiments are used. \n",
    "\n",
    "Eventhough the regressed model seems to be correct, we can never be entirely sure. The scattered results from the experiments does not necesarrily origin from just measurement errors, but can also come from errors in the model as process noise. What if there was actually no measurement error at all and the model is instead a function of not just velocity $v$ but also some hidden variable $u$: \n",
    "\n",
    "$$ f_y = \\beta \\cdot v + u $$ (eq_model_u)\n",
    "\n",
    "Comparing this expression with the one we used:\n",
    "\n",
    "$$ f_y = \\beta \\cdot v + \\epsilon $$ (eq_model_u)\n",
    "\n",
    "It can be realized that this model could also have created the data if $u=\\epsilon$. Which can also be seen in the python implementation below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747bdf9-f812-4e9a-9566-4a8ce16c50c8",
   "metadata": {
    "tags": [
     "hide_input"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "N = 10\n",
    "\n",
    "beta = 3\n",
    "gamma = 1\n",
    "scale = 0.5\n",
    "\n",
    "data2 = pd.DataFrame()\n",
    "u = data2['u'] = np.random.normal(loc = 0.0, scale = scale, size = N)\n",
    "v = data2['v'] = np.linspace(0,5,N)\n",
    "\n",
    "f_y = data2['f_y'] = beta*v + gamma*u\n",
    "f_y_measure = data2['f_y_measure'] = f_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d509c880-b9a1-4bfb-a85e-2388047073c6",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "fig,ax=plt.subplots()\n",
    "data2.plot(x='v', y='f_y_measure', style='bo', ax=ax);\n",
    "ax.set_ylabel(r'$f_y$')\n",
    "ax.set_xlabel(r'$v$');\n",
    "ax.get_legend().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4583729f-725c-4293-9851-3ab02bd12984",
   "metadata": {
    "tags": [
     "remove_input"
    ]
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.plot(data2['v'], data2['u'], data2['f_y_measure'], 'bo--',)\n",
    "ax.set_zlabel(r'$f_y$')\n",
    "ax.set_xlabel(r'$v$');\n",
    "ax.set_ylabel(r'$u$');\n",
    "ax.view_init(elev=50., azim=-120)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d3c70c-b166-49d5-ab2b-318b4d58124c",
   "metadata": {},
   "source": [
    "If $u$ is not included in the model it will end up in the process error $w$ so that the model is written:\n",
    "\n",
    "$$ f_y = \\beta \\cdot v + w $$ (eq_model_w)\n",
    "\n",
    "This is the model that we believe generated the data and we can observe this model by conducting experiments where also measurement error is introduced:\n",
    "\n",
    "$$ f_{y}^{measured} = f_y + \\epsilon $$ (eq_model_measure)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
